{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from io import StringIO\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list(url:str, types:str=None):\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data)\n",
    "    if types==\"protocol\":\n",
    "        df[\"protocol\"] = df[\"name\"].apply(lambda x: x.lower().replace(\" \", \"-\"))\n",
    "    df.to_excel(f\"{types}.xlsx\", index=False)\n",
    "    print(df.shape, df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url:str, keys:pd.Series, save_path:str=\"html\"):\n",
    "    options = Options()\n",
    "    # React로 만들어진 웹 페이지의 경우 headless 모드에서 \n",
    "    # javascript unenable로 인하여 데이터를 가져오지 못함\n",
    "    # options.add_argument(\"--headless\")  \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    for v in keys:\n",
    "        driver.get(url+v)\n",
    "        sleep(10)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        next_data_script = soup.find('script', id=\"__NEXT_DATA__\")\n",
    "        try:\n",
    "            data = next_data_script.string\n",
    "            data = json.loads(data)\n",
    "            if \"error\" in data[\"page\"]:\n",
    "                print(v, data[\"err\"])\n",
    "                continue\n",
    "            if not next_data_script.string:\n",
    "                print(v, \"id='__NEXT_DATA__'를 가진 script 태그를 찾을 수 없습니다.\")\n",
    "                continue\n",
    "            \n",
    "            data = next_data_script.string\n",
    "            try:\n",
    "                with open(f\"{save_path}/{v}.json\", \"w\") as f:\n",
    "                    f.write(data)\n",
    "            except UnicodeEncodeError as e:\n",
    "                with open(f\"{save_path}/{v}.json\", \"w\", encoding=\"utf8\") as f:\n",
    "                    f.write(data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(v, e)\n",
    "\n",
    "    driver.quit() # 1847 min at 20s sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protocol Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://defillama.com/protocol/\"\n",
    "get_data(url, df_protocol[\"protocol\"])\n",
    "# 1847 min at 20s sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://defillama.com/chain/\"\n",
    "get_data(url, df_chain[\"name\"], save_path=\"html_chain\")\n",
    "# 107min at 15s sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract TVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"df\", exist_ok=True)\n",
    "\n",
    "failed = {\n",
    "    \"Empty\": [],\n",
    "    \"OneTvl\": [],\n",
    "    \"UnicodeDecodeError\": [],\n",
    "    \"KeyError\": [],\n",
    "    \"Other\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_df(times, network):\n",
    "    times = times[\"tvl\"]\n",
    "    df = pd.DataFrame(times)\n",
    "    df[\"date\"] = df[\"date\"].apply(lambda x: datetime.fromtimestamp(x)) \n",
    "    df[network] = df[\"totalLiquidityUSD\"]\n",
    "    return df[[\"date\", network]]\n",
    "\n",
    "def is_filled(df_hist, df_not):\n",
    "    global failed\n",
    "\n",
    "    if df_hist is None and df_not is None:\n",
    "        print(f\"Empty tvl in {v}\")\n",
    "        return True\n",
    "    elif df_hist is None or df_not is None:\n",
    "        print(f\"Only one tvl in {v}: {df_hist.shape if df_hist else df_not.shape}\")\n",
    "        failed[\"OneTvl\"].append(v)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def is_identical(df_hist, df_not):\n",
    "    res = [\n",
    "        (c, (df_hist[c] != df_not[c]).sum())\n",
    "        for c in df_hist.columns\n",
    "    ]\n",
    "    result = (sum([v[-1] for v in res]) > 0)\n",
    "    if result:\n",
    "        print(f\"Different in {v}\")\n",
    "        print(res)\n",
    "    return not result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Protocol dataset to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in os.listdir(\"html\"):\n",
    "    try: \n",
    "        data = None\n",
    "        try:\n",
    "            with open(f\"html/{v}\", \"r\", encoding='utf8') as f:\n",
    "                data = f.read()\n",
    "        except UnicodeDecodeError as e:\n",
    "            try:\n",
    "                with open(f\"html/{v}\", \"r\", encoding='cp949') as f:\n",
    "                    data = f.read()\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "            \n",
    "        if not data:\n",
    "            print(f\"Empty in {v}\")\n",
    "            failed[\"Empty\"].append(v)\n",
    "            continue\n",
    "        data = json.loads(data)\n",
    "        data = data[\"props\"][\"pageProps\"][\"protocolData\"]\n",
    "\n",
    "        tvl = {\n",
    "            \"history\": {\n",
    "                \"data\": data[\"historicalChainTvls\"],\n",
    "                \"df\": None,\n",
    "            },\n",
    "            \"nothist\": {\n",
    "                \"data\": data[\"chainTvls\"],\n",
    "                \"df\": None,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for k in tvl.keys():\n",
    "            dfs = [\n",
    "                generate_df(times, network) \n",
    "                for network, times in tvl[k][\"data\"].items()\n",
    "                if times and times[\"tvl\"]\n",
    "            ]\n",
    "            \n",
    "            if len(dfs) == 1:\n",
    "                tvl[k][\"df\"] = dfs[0].fillna(0)\n",
    "            elif dfs:\n",
    "                result = dfs[0]\n",
    "                for d in dfs[1:]:\n",
    "                    result = pd.merge(result, d, on=\"date\", how=\"outer\")\n",
    "                tvl[k][\"df\"] = result.fillna(0)\n",
    "\n",
    "        if is_filled(tvl[\"history\"][\"df\"], tvl[\"nothist\"][\"df\"]):\n",
    "            continue\n",
    "\n",
    "        if is_identical(tvl[\"history\"][\"df\"], tvl[\"nothist\"][\"df\"]):\n",
    "            tvl[\"nothist\"][\"df\"].to_csv(\n",
    "                f\"df/{v.replace('.json', '.csv')}\", index=False)\n",
    "\n",
    "    except UnicodeDecodeError as e:\n",
    "        print(f\"UnicodeDecodeError in {v}: {e}\")\n",
    "        failed[\"UnicodeDecodeError\"].append(v)\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError in {v}: {e}\")\n",
    "        failed[\"KeyError\"].append(v)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {v}: {e}\")\n",
    "        failed[\"Other\"].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in os.listdir(\"df\"):\n",
    "    df = pd.read_csv(f\"df/{v}\")\n",
    "    df = df.iloc[:, 1:].fillna(0)\n",
    "    try:\n",
    "        if df.sum().sum() == 0:\n",
    "            os.remove(f\"df/{v}\")\n",
    "    except Exception as e:\n",
    "        print(v, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"result\", exist_ok=True)\n",
    "files = os.listdir(\"df\")\n",
    "\n",
    "\n",
    "for d in files:\n",
    "    df = pd.read_csv(f\"df/{d}\")\n",
    "    df[\"TotalTvl\"] = df.iloc[:, 1:].sum(axis=1)\n",
    "    if df[\"TotalTvl\"].sum() == 0:\n",
    "        continue\n",
    "    name = d[:-4]\n",
    "    df[name] = df[\"TotalTvl\"]\n",
    "    df.to_csv(f\"df/{d}\", index=False)\n",
    "\n",
    "    df = df[[\"date\", name]]\n",
    "    # 연속된 중복값은 처음, 마지막 값만 남기고 제거\n",
    "    df_filtered = df[(df[name] != df[name].shift(1)) | (df[name] != df[name].shift(-1))]\n",
    "    df_filtered.to_csv(f\"result/{d}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notc = [\"staking\", \"borrowed\", \"vesting\", \"offers\", \"pool2\", \"treasury\"]\n",
    "\n",
    "isin = lambda x: any([n in x for n in notc])\n",
    "\n",
    "for v in os.listdir(\"df\"):\n",
    "    df = pd.read_csv(f\"df/{v}\")\n",
    "    \n",
    "    # 첫 열 제외하고 가로 행이 모두 0인 행은 삭제\n",
    "    df = df.loc[(df.iloc[:, 1:] != 0).any(axis=1)]\n",
    "\n",
    "    cols = []\n",
    "    for c in df.columns[1:-2].values:\n",
    "        if not isin(c):\n",
    "            cols.append(c)\n",
    "    df[v[:-4]] = df[cols].sum(axis=1)\n",
    "    df.to_csv(f\"df/{v}\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain dataset to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in failed: # chain csv download에서의 failed list\n",
    "    v = v.replace(\"_\", \" \")\n",
    "    if not os.path.exists(f\"html/{v}.json\"):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(f\"html/{v}.json\", \"r\", encoding='utf8') as f:\n",
    "            data = f.read()\n",
    "    except UnicodeDecodeError as e:\n",
    "        with open(f\"html/{v}.json\", \"r\", encoding='cp949') as f:\n",
    "            data = f.read()\n",
    "\n",
    "    if not data:\n",
    "        continue\n",
    "    data = json.loads(data)\n",
    "    extra_data = data[\"props\"][\"pageProps\"][\"extraTvlCharts\"]\n",
    "    data = data[\"props\"][\"pageProps\"][\"chart\"]\n",
    "    data = pd.DataFrame({\n",
    "        \"date\": [int(x[0]) for x in data],\n",
    "        v: [x[1] for x in data]\n",
    "    })\n",
    "\n",
    "    temp = {}\n",
    "    for k, v in extra_data.items():\n",
    "        l = pd.DataFrame({\n",
    "            \"date\": [int(x[0]) for x in v],\n",
    "            k: [x[1] for x in v]\n",
    "        })\n",
    "        if l:\n",
    "            temp[k] = l\n",
    "\n",
    "    for k, v in temp.items():\n",
    "        data = pd.merge(data, v, on=\"date\", how=\"left\")\n",
    "\n",
    "    df[\"date\"] = df[\"date\"].apply(lambda x: datetime.fromtimestamp(x)) \n",
    "    df.to_csv(f'df_chain_web/{v.replace(\" \", \"_\")}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
